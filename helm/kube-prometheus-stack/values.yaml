---
crds:
  enabled: true
  upgradeJob:
    enabled: true
    # Conflicts with ArgoCD, force it, ArgoCD will fix it later
    forceConflicts: true

sidecar:
  dashboards:
    enabled: true

grafana:
  enabled: true
  defaultDashboardsTimezone: Europe/Rome

  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: default
          orgId: 1
          folder: ""
          type: file
          disableDeletion: true
          editable: false
          options:
            path: /var/lib/grafana/dashboards/default

  dashboards:
    default:
      argocd:
        url: "https://raw.githubusercontent.com/argoproj/argo-cd/refs/heads/master/examples/dashboard.json"
        datasource: Prometheus
      cloudnative-pg:
        gnetId: 20417
        revision: 4
        datasource: Prometheus
      smartctl-exporter:
        url: "https://raw.githubusercontent.com/blesswinsamuel/grafana-dashboards/refs/heads/main/dashboards/dist/dashboards/smartctl.json"
        datasource: Prometheus
      velero:
        gnetId: 16829
        revision: 5
        datasource: Prometheus
      kubernetes-views-global:
        gnetId: 15757
        revision: 43
        datasource: Prometheus
      longhorn:
        gnetId: 16888
        revision: 9
        datasource: Prometheus
      node-exporter-temperature:
        gnetId: 15202
        revision: 1
        datasource: Prometheus

prometheusOperator:
  tls:
    enabled: false
  admissionWebhooks:
    enabled: false

prometheus:
  enabled: true

  prometheusSpec:
    ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the servicemonitors created
    serviceMonitorSelectorNilUsesHelmValues: false

    ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the podmonitors created
    podMonitorSelectorNilUsesHelmValues: false

    ## External URL at which Prometheus will be reachable.
    ##
    externalUrl: "https://prometheus.kalexlab.xyz"

    storageSpec:
      volumeClaimTemplate:
        metadata:
          name: prometheus-data-pvc
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi

    thanosService:
      enabled: true

    thanosServiceMonitor:
      enabled: true

alertmanager:
  enabled: true
  config:
    global:
      resolve_timeout: 5m

    route:
      receiver: "null"
      routes:
        - match:
            severity: "critical"
          receiver: "telegram"
          group_by: ["alertname", "severity"]
          group_wait: 30s
          group_interval: 5m
          repeat_interval: 24h

coreDns:
  # Service exposed by coreDns itself, appears to not be working otherwise
  service:
    selector:
      app.kubernetes.io/name: coredns

kubeEtcd:
  # Control plane virtual IP from talos/machine/vip.yaml and talos/talenv.prod.yaml
  endpoints:
    - 192.168.10.9

  service:
    enabled: true
    port: 2381
    targetPort: 2381

kubeProxy:
  enabled: false

defaultRules:
  disabled:
    KubeProxyDown: true
