---
crds:
  enabled: true
  upgradeJob:
    enabled: true
    # Conflicts with ArgoCD, force it, ArgoCD will fix it later
    forceConflicts: true

sidecar:
  dashboards:
    enabled: true

extraManifests:
  - apiVersion: v1
    kind: Secret
    metadata:
      name: grafana-oidc-client-secret
      namespace: prometheus
    data:
      client_secret: "{{ .Values.secrets.client_secret | b64enc }}"

grafana:
  enabled: true

  defaultDashboardsTimezone: Europe/Rome

  grafana.ini:
    server:
      root_url: "https://grafana.kalexlab.xyz"

    auth.generic_oauth:
      enabled: true
      name: Authelia
      client_id: vP36vqAl-kDRRlq0~88S1u-2jdyyktDkLDuPsHmwHp0oDnuBRg-KH4DeJXaAWXBXU4.uL7YK
      client_secret: "$__file{/etc/secrets/grafana_oidc_client_secret/client_secret}"
      scopes: openid profile email groups
      empty_scopes: false
      login_attribute_path: preferred_username
      groups_attribute_path: groups
      name_attribute_path: name
      role_attribute_path: contains(groups[*], 'admins') && 'Admin' || 'Viewer'
      use_pkce: true

  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: default
          orgId: 1
          folder: ""
          type: file
          disableDeletion: true
          editable: false
          options:
            path: /var/lib/grafana/dashboards/default

  dashboards:
    default:
      argocd:
        url: "https://raw.githubusercontent.com/argoproj/argo-cd/refs/heads/master/examples/dashboard.json"
        datasource: Prometheus
      cloudnative-pg:
        gnetId: 20417
        revision: 4
        datasource: Prometheus
      smartctl-exporter:
        url: "https://raw.githubusercontent.com/blesswinsamuel/grafana-dashboards/refs/heads/main/dashboards/dist/dashboards/smartctl.json"
        datasource: Prometheus
      velero:
        gnetId: 16829
        revision: 5
        datasource: Prometheus
      kubernetes-views-global:
        gnetId: 15757
        revision: 43
        datasource: Prometheus
      longhorn:
        url: "https://raw.githubusercontent.com/onzack/grafana-dashboards/refs/heads/main/grafana/longhorn/onzack-longhorn-monitoring.json"
      node-exporter-temperature:
        gnetId: 15202
        revision: 1
        datasource: Prometheus
      traefik:
        gnetId: 17346
        revision: 9
        datasource: Prometheus

  extraSecretMounts:
    - name: grafana-oidc-client-secret
      secretName: grafana-oidc-client-secret
      mountPath: /etc/secrets/grafana_oidc_client_secret

prometheusOperator:
  tls:
    enabled: false
  admissionWebhooks:
    enabled: false

prometheus:
  enabled: true

  prometheusSpec:
    ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the servicemonitors created
    serviceMonitorSelectorNilUsesHelmValues: false

    ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the podmonitors created
    podMonitorSelectorNilUsesHelmValues: false

    ## External URL at which Prometheus will be reachable.
    ##
    externalUrl: "https://prometheus.kalexlab.xyz"

    storageSpec:
      volumeClaimTemplate:
        metadata:
          name: prometheus-data-pvc
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi

    thanosService:
      enabled: true

    thanosServiceMonitor:
      enabled: true

    # Bugged, not added to prometheus.monitoring.coreos.com/prometheus-kube-prometheus-prometheus even though it really should
    # additionalScrapeConfigs:
    #   - job_name: "kubernetes-pods"
    #     honor_labels: true
    #
    #     kubernetes_sd_configs:
    #       - role: pod
    #
    #     relabel_configs:
    #       - source_labels:
    #           [
    #             __meta_kubernetes_pod_annotation_observability_kalexlab_xyz_scrape,
    #           ]
    #         action: keep
    #         regex: true
    #       - source_labels:
    #           [
    #             __meta_kubernetes_pod_annotation_observability_kalexlab_xyz_scrape,
    #           ]
    #         action: replace
    #         regex: (https?)
    #         target_label: __scheme__
    #       - source_labels:
    #           [__meta_kubernetes_pod_annotation_observability_kalexlab_xyz_path]
    #         action: replace
    #         target_label: __metrics_path__
    #         regex: (.+)
    #       - source_labels:
    #           [
    #             __address__,
    #             __meta_kubernetes_pod_annotation_observability_kalexlab_xyz_port,
    #           ]
    #         action: replace
    #         regex: (.+?)(?::\d+)?;(\d+)
    #         replacement: $1:$2
    #         target_label: __address__
    #       - source_labels: [__meta_kubernetes_pod_phase]
    #         regex: Pending|Succeeded|Failed|Completed
    #         action: drop

alertmanager:
  enabled: true
  config:
    global:
      resolve_timeout: 5m

    route:
      receiver: "null"
      routes:
        - match:
            severity: "critical"
          receiver: "telegram"
          group_by: ["alertname", "severity"]
          group_wait: 30s
          group_interval: 5m
          repeat_interval: 24h

coreDns:
  # Service exposed by coreDns itself, appears to not be working otherwise
  service:
    selector:
      app.kubernetes.io/name: coredns

kubeEtcd:
  # Control plane virtual IP from talos/machine/vip.yaml and talos/talenv.prod.yaml
  endpoints:
    - 192.168.10.9

  service:
    enabled: true
    port: 2381
    targetPort: 2381

kubeProxy:
  enabled: false

defaultRules:
  disabled:
    KubeProxyDown: true
